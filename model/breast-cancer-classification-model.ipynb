{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Breast Cancer Classification </center>\n",
    "\n",
    "<b>Problem statement: </b> In this project we are going to classify patients abnormal mass of cells (tumors). Tumors can either be benign (non-cancerous) or malignant (cancerous).Benign tumors are generally not harmful but if a patient is diagnosed with a malignant tumor then they should be given treatment immediately. This is a classification problem with two classes to predict. We will start with logistic regression as our model and eventually select the best model for our dataset.\n",
    "\n",
    "\n",
    "An Otsogile Ogaisitse Onalepelo aka Morena project.\n",
    "\n",
    "\n",
    "### Import the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from sklearn\n",
    "breast_cancer_dataset = sklearn.datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
      "        1.189e-01],\n",
      "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
      "        8.902e-02],\n",
      "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
      "        8.758e-02],\n",
      "       ...,\n",
      "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
      "        7.820e-02],\n",
      "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
      "        1.240e-01],\n",
      "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
      "        7.039e-02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), 'frame': None, 'target_names': array(['malignant', 'benign'], dtype='<U9'), 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.', 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
      "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
      "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
      "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
      "       'smoothness error', 'compactness error', 'concavity error',\n",
      "       'concave points error', 'symmetry error',\n",
      "       'fractal dimension error', 'worst radius', 'worst texture',\n",
      "       'worst perimeter', 'worst area', 'worst smoothness',\n",
      "       'worst compactness', 'worst concavity', 'worst concave points',\n",
      "       'worst symmetry', 'worst fractal dimension'], dtype='<U23'), 'filename': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\breast_cancer.csv'}\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the feature names\n",
    "breast_cancer_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into a dataframe\n",
    "data_frame = pd.DataFrame(breast_cancer_dataset.data, columns = breast_cancer_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows of the dataframe\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the 'target' column to the data frame\n",
    "data_frame['label'] = breast_cancer_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "564                0.2216          0.2060                  0.07115      0  \n",
       "565                0.1628          0.2572                  0.06637      0  \n",
       "566                0.1418          0.2218                  0.07820      0  \n",
       "567                0.2650          0.4087                  0.12400      0  \n",
       "568                0.0000          0.2871                  0.07039      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print last 5 rows of the dataframe\n",
    "data_frame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print number of rows and columns in the dataset\n",
    "data_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 569 rows and 31 columns in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  label                    569 non-null    int32  \n",
      "dtypes: float64(30), int32(1)\n",
      "memory usage: 135.7 KB\n"
     ]
    }
   ],
   "source": [
    "# getting some information about the data\n",
    "data_frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we do not have any missing values in our data. All our features have float64 data type other than the target feature .i.e. label which have an int32 data type. We also do not have any categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                0\n",
       "mean texture               0\n",
       "mean perimeter             0\n",
       "mean area                  0\n",
       "mean smoothness            0\n",
       "mean compactness           0\n",
       "mean concavity             0\n",
       "mean concave points        0\n",
       "mean symmetry              0\n",
       "mean fractal dimension     0\n",
       "radius error               0\n",
       "texture error              0\n",
       "perimeter error            0\n",
       "area error                 0\n",
       "smoothness error           0\n",
       "compactness error          0\n",
       "concavity error            0\n",
       "concave points error       0\n",
       "symmetry error             0\n",
       "fractal dimension error    0\n",
       "worst radius               0\n",
       "worst texture              0\n",
       "worst perimeter            0\n",
       "worst area                 0\n",
       "worst smoothness           0\n",
       "worst compactness          0\n",
       "worst concavity            0\n",
       "worst concave points       0\n",
       "worst symmetry             0\n",
       "worst fractal dimension    0\n",
       "label                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values again\n",
    "data_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaay!ðŸ˜ƒðŸ˜ƒ We really do not have any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
       "count     569.000000              569.000000  ...     569.000000   \n",
       "mean        0.181162                0.062798  ...      25.677223   \n",
       "std         0.027414                0.007060  ...       6.146258   \n",
       "min         0.106000                0.049960  ...      12.020000   \n",
       "25%         0.161900                0.057700  ...      21.080000   \n",
       "50%         0.179200                0.061540  ...      25.410000   \n",
       "75%         0.195700                0.066120  ...      29.720000   \n",
       "max         0.304000                0.097440  ...      49.540000   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       worst fractal dimension       label  \n",
       "count               569.000000  569.000000  \n",
       "mean                  0.083946    0.627417  \n",
       "std                   0.018061    0.483918  \n",
       "min                   0.055040    0.000000  \n",
       "25%                   0.071460    0.000000  \n",
       "50%                   0.080040    1.000000  \n",
       "75%                   0.092080    1.000000  \n",
       "max                   0.207500    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistical measures about the data\n",
    "data_frame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm....The following features have very huge values while others have small values.\n",
    "\n",
    "-mean radius\n",
    "\n",
    "-mean texture\n",
    "\n",
    "-mean perimeter\n",
    "\n",
    "-mean area\n",
    "\n",
    "-worst texture\n",
    "\n",
    "-worst perimeter\n",
    "\n",
    "-worst area\n",
    "\n",
    "-area error\n",
    "\n",
    "We are going to have to scale them in a bit so we have values in our dataset within a common range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of Target Varibale\n",
    "data_frame['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 --> Benign\n",
    "\n",
    "0 --> Malignant\n",
    "\n",
    "There is also an imbalance in our classes. We have more benign observations with 357 compared to 212 for malignant. This is another problem we have to resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.462830</td>\n",
       "      <td>21.604906</td>\n",
       "      <td>115.365377</td>\n",
       "      <td>978.376415</td>\n",
       "      <td>0.102898</td>\n",
       "      <td>0.145188</td>\n",
       "      <td>0.160775</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.192909</td>\n",
       "      <td>0.062680</td>\n",
       "      <td>...</td>\n",
       "      <td>21.134811</td>\n",
       "      <td>29.318208</td>\n",
       "      <td>141.370330</td>\n",
       "      <td>1422.286321</td>\n",
       "      <td>0.144845</td>\n",
       "      <td>0.374824</td>\n",
       "      <td>0.450606</td>\n",
       "      <td>0.182237</td>\n",
       "      <td>0.323468</td>\n",
       "      <td>0.091530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.146524</td>\n",
       "      <td>17.914762</td>\n",
       "      <td>78.075406</td>\n",
       "      <td>462.790196</td>\n",
       "      <td>0.092478</td>\n",
       "      <td>0.080085</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.025717</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.062867</td>\n",
       "      <td>...</td>\n",
       "      <td>13.379801</td>\n",
       "      <td>23.515070</td>\n",
       "      <td>87.005938</td>\n",
       "      <td>558.899440</td>\n",
       "      <td>0.124959</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.166238</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.270246</td>\n",
       "      <td>0.079442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter   mean area  mean smoothness  \\\n",
       "label                                                                           \n",
       "0        17.462830     21.604906      115.365377  978.376415         0.102898   \n",
       "1        12.146524     17.914762       78.075406  462.790196         0.092478   \n",
       "\n",
       "       mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "label                                                                         \n",
       "0              0.145188        0.160775             0.087990       0.192909   \n",
       "1              0.080085        0.046058             0.025717       0.174186   \n",
       "\n",
       "       mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "label                          ...                                \n",
       "0                    0.062680  ...     21.134811      29.318208   \n",
       "1                    0.062867  ...     13.379801      23.515070   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "label                                                                      \n",
       "0           141.370330  1422.286321          0.144845           0.374824   \n",
       "1            87.005938   558.899440          0.124959           0.182673   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "label                                                          \n",
       "0             0.450606              0.182237        0.323468   \n",
       "1             0.166238              0.074444        0.270246   \n",
       "\n",
       "       worst fractal dimension  \n",
       "label                           \n",
       "0                     0.091530  \n",
       "1                     0.079442  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.groupby('label').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Feature Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.338222</td>\n",
       "      <td>0.323965</td>\n",
       "      <td>0.332935</td>\n",
       "      <td>0.216920</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363998</td>\n",
       "      <td>0.283138</td>\n",
       "      <td>0.170906</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.166787</td>\n",
       "      <td>0.145453</td>\n",
       "      <td>0.167915</td>\n",
       "      <td>0.149274</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163813</td>\n",
       "      <td>0.167352</td>\n",
       "      <td>0.139932</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.223342</td>\n",
       "      <td>0.218465</td>\n",
       "      <td>0.216847</td>\n",
       "      <td>0.117413</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241471</td>\n",
       "      <td>0.167837</td>\n",
       "      <td>0.081130</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.302381</td>\n",
       "      <td>0.308759</td>\n",
       "      <td>0.293345</td>\n",
       "      <td>0.172895</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356876</td>\n",
       "      <td>0.235320</td>\n",
       "      <td>0.123206</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.416442</td>\n",
       "      <td>0.408860</td>\n",
       "      <td>0.416765</td>\n",
       "      <td>0.271135</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471748</td>\n",
       "      <td>0.373475</td>\n",
       "      <td>0.220901</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter   mean area  mean smoothness  \\\n",
       "count   569.000000    569.000000      569.000000  569.000000       569.000000   \n",
       "mean      0.338222      0.323965        0.332935    0.216920         0.096360   \n",
       "std       0.166787      0.145453        0.167915    0.149274         0.014064   \n",
       "min       0.000000      0.000000        0.000000    0.000000         0.052630   \n",
       "25%       0.223342      0.218465        0.216847    0.117413         0.086370   \n",
       "50%       0.302381      0.308759        0.293345    0.172895         0.095870   \n",
       "75%       0.416442      0.408860        0.416765    0.271135         0.105300   \n",
       "max       1.000000      1.000000        1.000000    1.000000         0.163400   \n",
       "\n",
       "       mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "count        569.000000      569.000000           569.000000     569.000000   \n",
       "mean           0.104341        0.088799             0.048919       0.181162   \n",
       "std            0.052813        0.079720             0.038803       0.027414   \n",
       "min            0.019380        0.000000             0.000000       0.106000   \n",
       "25%            0.064920        0.029560             0.020310       0.161900   \n",
       "50%            0.092630        0.061540             0.033500       0.179200   \n",
       "75%            0.130400        0.130700             0.074000       0.195700   \n",
       "max            0.345400        0.426800             0.201200       0.304000   \n",
       "\n",
       "       mean fractal dimension  ...  worst texture  worst perimeter  \\\n",
       "count              569.000000  ...     569.000000       569.000000   \n",
       "mean                 0.062798  ...       0.363998         0.283138   \n",
       "std                  0.007060  ...       0.163813         0.167352   \n",
       "min                  0.049960  ...       0.000000         0.000000   \n",
       "25%                  0.057700  ...       0.241471         0.167837   \n",
       "50%                  0.061540  ...       0.356876         0.235320   \n",
       "75%                  0.066120  ...       0.471748         0.373475   \n",
       "max                  0.097440  ...       1.000000         1.000000   \n",
       "\n",
       "       worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "count  569.000000        569.000000         569.000000       569.000000   \n",
       "mean     0.170906          0.132369           0.254265         0.272188   \n",
       "std      0.139932          0.022832           0.157336         0.208624   \n",
       "min      0.000000          0.071170           0.027290         0.000000   \n",
       "25%      0.081130          0.116600           0.147200         0.114500   \n",
       "50%      0.123206          0.131300           0.211900         0.226700   \n",
       "75%      0.220901          0.146000           0.339100         0.382900   \n",
       "max      1.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       worst concave points  worst symmetry  worst fractal dimension  \\\n",
       "count            569.000000      569.000000               569.000000   \n",
       "mean               0.114606        0.290076                 0.083946   \n",
       "std                0.065732        0.061867                 0.018061   \n",
       "min                0.000000        0.156500                 0.055040   \n",
       "25%                0.064930        0.250400                 0.071460   \n",
       "50%                0.099930        0.282200                 0.080040   \n",
       "75%                0.161400        0.317900                 0.092080   \n",
       "max                0.291000        0.663800                 0.207500   \n",
       "\n",
       "            label  \n",
       "count  569.000000  \n",
       "mean     0.627417  \n",
       "std      0.483918  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols_to_scale = ['mean radius','mean texture','mean perimeter','mean area','worst texture','worst perimeter','worst area','area error']\n",
    "scaler = MinMaxScaler()\n",
    "data_frame[cols_to_scale] = scaler.fit_transform(data_frame[cols_to_scale])\n",
    "data_frame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Nice!. I really love what i am looking at here..ðŸ˜. Now we have our feature values ranging from 0 to 1,some just a few points above 1 and others below but not by much. Which is better compared to how they were initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Dataset Imbalance\n",
    "\n",
    "Training a model on an imbalanced dataset might result in our model not performing as per our expectations. Therefore, before we go any further with our data preprocessing. Let us handle this imbalance first. There are various techniques we could utilize to accomplish this like, undersampling the majority class, oversampling the minority class, ensenble method and SMOTE. For this pipeline we will oversample the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "1    357\n",
      "0    357\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class count\n",
    "count_class_1, count_class_0 = data_frame.label.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = data_frame[data_frame['label'] == 0]\n",
    "df_class_1 = data_frame[data_frame['label'] == 1]\n",
    "\n",
    "# Oversample 0-class and concat the DataFrames of both classes\n",
    "df_class_0_over = df_class_0.sample(count_class_1, replace=True)\n",
    "df_test_over = pd.concat([df_class_0_over, df_class_1], axis=0)\n",
    "\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravooo ðŸ‘ðŸ‘. Our classes are now perfectly balanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and Removal\n",
    "\n",
    "We kind of have the curse of dimensionality in this dataset therefore, using traditional and statistical outlier detection methods is slightly not practical. We will have to go over each feature and analyse it. Thus, we will use a automatic outlier detection and removal technique specifically One Class SVM.\n",
    "\n",
    "\n",
    "For more information on automatic outlier detection methods read more here: https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/. Some code used here in is kind of borrowed from there.ðŸ˜†ðŸ˜…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499, 30) (499,)\n",
      "(491, 30) (491,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# retrieve the array\n",
    "data = df_test_over.values\n",
    "# split into input and output elements to prevent over fitting and data leakage\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "#The class provides the â€œnuâ€ argument that specifies a approx ratio of outliers in the dataset, default = 0.1\n",
    "ocs = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.01)\n",
    "yhat = ocs.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! So, different nu values detects and removes a different number of outliers in our dataset which might change our class value counts as well. Maybe we should have balanced the dataset after removing outliers? ðŸ¤”ðŸ¤”ðŸ¤”.Probably!! Anyhow, lets leave it as it is and proceed to feature selection.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "We have 30 features in our dataset. Some of them might actually be very important for building our model. Therefore let us use mutual information to detect the most important features and select them for our model.\n",
    "\n",
    "\n",
    "#### Mutual Information  (MI)\n",
    "\n",
    "MI Estimate mutual information for a discrete target variable.\n",
    "\n",
    "Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency between variables.\n",
    "\n",
    "The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "Inshort\n",
    "\n",
    "A quantity called mutual information measures the amount of information one can obtain from one random variable given another.\n",
    "\n",
    "The mutual information between two random variables X and Y can be stated formally as follows:\n",
    "\n",
    "<b>I(X ; Y) = H(X) â€“ H(X | Y)<b>\n",
    "Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38338306, 0.20921632, 0.44297747, 0.42665888, 0.14974928,\n",
       "       0.27087161, 0.43113969, 0.53936594, 0.13411706, 0.1167293 ,\n",
       "       0.33198994, 0.08277302, 0.28038738, 0.39022421, 0.09220824,\n",
       "       0.1469117 , 0.19497837, 0.2154982 , 0.14046555, 0.09607136,\n",
       "       0.4972071 , 0.21504626, 0.52828156, 0.49665339, 0.2178502 ,\n",
       "       0.28000312, 0.37604727, 0.4980107 , 0.26021335, 0.16367769])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the library that we will be using to do feature selection using mutual information for classification problems\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "# determine the mutual information\n",
    "mutual_info = mutual_info_classif(X_train, y_train)\n",
    "mutual_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the mutual info class you never get back any negative values. If you get a very high value, then it means that feature dependency on the target variable is too much and which ever features have high values are the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean concave points        0.539366\n",
       "worst perimeter            0.528282\n",
       "worst concave points       0.498011\n",
       "worst radius               0.497207\n",
       "worst area                 0.496653\n",
       "mean perimeter             0.442977\n",
       "mean concavity             0.431140\n",
       "mean area                  0.426659\n",
       "area error                 0.390224\n",
       "mean radius                0.383383\n",
       "worst concavity            0.376047\n",
       "radius error               0.331990\n",
       "perimeter error            0.280387\n",
       "worst compactness          0.280003\n",
       "mean compactness           0.270872\n",
       "worst symmetry             0.260213\n",
       "worst smoothness           0.217850\n",
       "concave points error       0.215498\n",
       "worst texture              0.215046\n",
       "mean texture               0.209216\n",
       "concavity error            0.194978\n",
       "worst fractal dimension    0.163678\n",
       "mean smoothness            0.149749\n",
       "compactness error          0.146912\n",
       "symmetry error             0.140466\n",
       "mean symmetry              0.134117\n",
       "mean fractal dimension     0.116729\n",
       "fractal dimension error    0.096071\n",
       "smoothness error           0.092208\n",
       "texture error              0.082773\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the mutual information array into a series so we can see which value belongs to which feature.\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = breast_cancer_dataset.feature_names\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that worst area has the highest dependency on our target variable followed by worst perimeter etc. Its not always necessary that you should take all the features for your model. You can take the top 10 or 20 features and drop others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20066cab908>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's plot the ordered mutual_info values per feature\n",
    "mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean compactness', 'mean concavity', 'mean concave points',\n",
       "       'radius error', 'perimeter error', 'area error',\n",
       "       'concave points error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry'], dtype='<U23')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this module allows us to select our desired features per the percentage or criteria we want e.g. top 10, top 10%, top 10 percentile of the features etc\n",
    "#this is also usually determied by the number of features in our dataset\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "#Now we Will select the  top 20 features\n",
    "#To avoid overfitting and data leak. We only do this process for X  and y Train\n",
    "sel_twenty_cols = SelectKBest(mutual_info_classif, k=20)\n",
    "sel_twenty_cols.fit(X_train, y_train)\n",
    "#display the top5 features we can use for our model building\n",
    "breast_cancer_dataset.feature_names[sel_twenty_cols.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_twenty_features = ['mean radius', 'mean perimeter', 'mean area', 'mean smoothness',\n",
    "       'mean compactness', 'mean concavity', 'mean concave points',\n",
    "       'radius error', 'perimeter error', 'area error', 'concavity error',\n",
    "       'concave points error', 'worst radius', 'worst texture',\n",
    "       'worst perimeter', 'worst area', 'worst smoothness',\n",
    "       'worst compactness', 'worst concavity', 'worst concave points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put X_train and X_test back into dataframes for feature selection\n",
    "X_train_df =pd.DataFrame(X_train,columns=breast_cancer_dataset.feature_names)\n",
    "X_test_df =pd.DataFrame(X_test,columns=breast_cancer_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the X_train and X_test above because we have already removed outliers from the main data frame, it will be a step back if we used it as we will have to remove ourliers again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.402717</td>\n",
       "      <td>0.346973</td>\n",
       "      <td>0.405017</td>\n",
       "      <td>0.255016</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.15620</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.09113</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.06744</td>\n",
       "      <td>...</td>\n",
       "      <td>21.20</td>\n",
       "      <td>0.463486</td>\n",
       "      <td>0.456646</td>\n",
       "      <td>0.288488</td>\n",
       "      <td>0.1681</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.5553</td>\n",
       "      <td>0.21210</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.607175</td>\n",
       "      <td>0.420697</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.473595</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.14790</td>\n",
       "      <td>0.09498</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>27.32</td>\n",
       "      <td>0.502665</td>\n",
       "      <td>0.679267</td>\n",
       "      <td>0.543846</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.448625</td>\n",
       "      <td>0.351708</td>\n",
       "      <td>0.452699</td>\n",
       "      <td>0.292428</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.17930</td>\n",
       "      <td>0.08866</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.06323</td>\n",
       "      <td>...</td>\n",
       "      <td>17.79</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>0.364012</td>\n",
       "      <td>0.195635</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.20350</td>\n",
       "      <td>0.3054</td>\n",
       "      <td>0.09519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.296228</td>\n",
       "      <td>0.352384</td>\n",
       "      <td>0.297699</td>\n",
       "      <td>0.169417</td>\n",
       "      <td>0.08284</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.02833</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.06432</td>\n",
       "      <td>...</td>\n",
       "      <td>15.44</td>\n",
       "      <td>0.359275</td>\n",
       "      <td>0.321679</td>\n",
       "      <td>0.134757</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.5646</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.2845</td>\n",
       "      <td>0.12490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247480</td>\n",
       "      <td>0.148123</td>\n",
       "      <td>0.241794</td>\n",
       "      <td>0.135101</td>\n",
       "      <td>0.08108</td>\n",
       "      <td>0.07823</td>\n",
       "      <td>0.06839</td>\n",
       "      <td>0.02534</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>0.06154</td>\n",
       "      <td>...</td>\n",
       "      <td>13.13</td>\n",
       "      <td>0.193763</td>\n",
       "      <td>0.185467</td>\n",
       "      <td>0.084718</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3076</td>\n",
       "      <td>0.09140</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.08824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.267358</td>\n",
       "      <td>0.373690</td>\n",
       "      <td>0.265082</td>\n",
       "      <td>0.142906</td>\n",
       "      <td>0.09933</td>\n",
       "      <td>0.12090</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>0.06021</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.07070</td>\n",
       "      <td>...</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.358475</td>\n",
       "      <td>0.192191</td>\n",
       "      <td>0.084103</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.323678</td>\n",
       "      <td>0.499831</td>\n",
       "      <td>0.335429</td>\n",
       "      <td>0.191898</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.16810</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.06759</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.07237</td>\n",
       "      <td>...</td>\n",
       "      <td>16.01</td>\n",
       "      <td>0.557569</td>\n",
       "      <td>0.276856</td>\n",
       "      <td>0.148152</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.3966</td>\n",
       "      <td>0.3381</td>\n",
       "      <td>0.15210</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.11830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.352075</td>\n",
       "      <td>0.340210</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.211665</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.09388</td>\n",
       "      <td>0.05839</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.06390</td>\n",
       "      <td>...</td>\n",
       "      <td>16.33</td>\n",
       "      <td>0.502132</td>\n",
       "      <td>0.294288</td>\n",
       "      <td>0.157589</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.3026</td>\n",
       "      <td>0.3194</td>\n",
       "      <td>0.15650</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0.09353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.252686</td>\n",
       "      <td>0.090632</td>\n",
       "      <td>0.242278</td>\n",
       "      <td>0.135992</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.1959</td>\n",
       "      <td>0.05955</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>0.096482</td>\n",
       "      <td>0.182081</td>\n",
       "      <td>0.089437</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.385205</td>\n",
       "      <td>0.235712</td>\n",
       "      <td>0.380001</td>\n",
       "      <td>0.243097</td>\n",
       "      <td>0.08876</td>\n",
       "      <td>0.09588</td>\n",
       "      <td>0.07550</td>\n",
       "      <td>0.04079</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.05986</td>\n",
       "      <td>...</td>\n",
       "      <td>17.77</td>\n",
       "      <td>0.219083</td>\n",
       "      <td>0.335126</td>\n",
       "      <td>0.197675</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.3327</td>\n",
       "      <td>0.12520</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>0.09740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0       0.402717      0.346973        0.405017   0.255016          0.11600   \n",
       "1       0.607175      0.420697        0.595743   0.473595          0.09831   \n",
       "2       0.448625      0.351708        0.452699   0.292428          0.09831   \n",
       "3       0.296228      0.352384        0.297699   0.169417          0.08284   \n",
       "4       0.247480      0.148123        0.241794   0.135101          0.08108   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "486     0.267358      0.373690        0.265082   0.142906          0.09933   \n",
       "487     0.323678      0.499831        0.335429   0.191898          0.11620   \n",
       "488     0.352075      0.340210        0.350287   0.211665          0.09752   \n",
       "489     0.252686      0.090632        0.242278   0.135992          0.10280   \n",
       "490     0.385205      0.235712        0.380001   0.243097          0.08876   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.15620         0.18910              0.09113         0.1929   \n",
       "1             0.10270         0.14790              0.09498         0.1582   \n",
       "2             0.15560         0.17930              0.08866         0.1794   \n",
       "3             0.12230         0.10100              0.02833         0.1601   \n",
       "4             0.07823         0.06839              0.02534         0.1646   \n",
       "..                ...             ...                  ...            ...   \n",
       "486           0.12090         0.10650              0.06021         0.1735   \n",
       "487           0.16810         0.13570              0.06759         0.2275   \n",
       "488           0.11410         0.09388              0.05839         0.1879   \n",
       "489           0.06981         0.03987              0.03700         0.1959   \n",
       "490           0.09588         0.07550              0.04079         0.1594   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.06744  ...         21.20       0.463486   \n",
       "1                   0.05395  ...         27.32       0.502665   \n",
       "2                   0.06323  ...         17.79       0.437900   \n",
       "3                   0.06432  ...         15.44       0.359275   \n",
       "4                   0.06154  ...         13.13       0.193763   \n",
       "..                      ...  ...           ...            ...   \n",
       "486                 0.07070  ...         13.33       0.358475   \n",
       "487                 0.07237  ...         16.01       0.557569   \n",
       "488                 0.06390  ...         16.33       0.502132   \n",
       "489                 0.05955  ...         13.50       0.096482   \n",
       "490                 0.05986  ...         17.77       0.219083   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0           0.456646    0.288488            0.1681             0.3913   \n",
       "1           0.679267    0.543846            0.1512             0.3150   \n",
       "2           0.364012    0.195635            0.1415             0.4667   \n",
       "3           0.321679    0.134757            0.1201             0.5646   \n",
       "4           0.185467    0.084718            0.1026             0.2431   \n",
       "..               ...         ...               ...                ...   \n",
       "486         0.192191    0.084103            0.1287             0.2250   \n",
       "487         0.276856    0.148152            0.1794             0.3966   \n",
       "488         0.294288    0.157589            0.1431             0.3026   \n",
       "489         0.182081    0.089437            0.1385             0.1266   \n",
       "490         0.335126    0.197675            0.1491             0.3331   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.5553               0.21210          0.3187   \n",
       "1             0.5372               0.23880          0.2768   \n",
       "2             0.5862               0.20350          0.3054   \n",
       "3             0.6556               0.13570          0.2845   \n",
       "4             0.3076               0.09140          0.2677   \n",
       "..               ...                   ...             ...   \n",
       "486           0.2216               0.11050          0.2226   \n",
       "487           0.3381               0.15210          0.3651   \n",
       "488           0.3194               0.15650          0.2718   \n",
       "489           0.1242               0.09391          0.2827   \n",
       "490           0.3327               0.12520          0.3415   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.10190  \n",
       "1                    0.07615  \n",
       "2                    0.09519  \n",
       "3                    0.12490  \n",
       "4                    0.08824  \n",
       "..                       ...  \n",
       "486                  0.08486  \n",
       "487                  0.11830  \n",
       "488                  0.09353  \n",
       "489                  0.06771  \n",
       "490                  0.09740  \n",
       "\n",
       "[491 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.334564</td>\n",
       "      <td>0.589787</td>\n",
       "      <td>0.328865</td>\n",
       "      <td>0.193807</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>15.30</td>\n",
       "      <td>0.563699</td>\n",
       "      <td>0.247971</td>\n",
       "      <td>0.128170</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.412656</td>\n",
       "      <td>0.358471</td>\n",
       "      <td>0.396724</td>\n",
       "      <td>0.264305</td>\n",
       "      <td>0.09597</td>\n",
       "      <td>0.08799</td>\n",
       "      <td>0.065930</td>\n",
       "      <td>0.051890</td>\n",
       "      <td>0.1618</td>\n",
       "      <td>0.05549</td>\n",
       "      <td>...</td>\n",
       "      <td>20.11</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>0.392898</td>\n",
       "      <td>0.266368</td>\n",
       "      <td>0.14140</td>\n",
       "      <td>0.35470</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>0.08631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268304</td>\n",
       "      <td>0.286101</td>\n",
       "      <td>0.268813</td>\n",
       "      <td>0.145111</td>\n",
       "      <td>0.10760</td>\n",
       "      <td>0.13340</td>\n",
       "      <td>0.080170</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.06854</td>\n",
       "      <td>...</td>\n",
       "      <td>14.38</td>\n",
       "      <td>0.269989</td>\n",
       "      <td>0.223517</td>\n",
       "      <td>0.110229</td>\n",
       "      <td>0.15330</td>\n",
       "      <td>0.38420</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.3230</td>\n",
       "      <td>0.10330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.187373</td>\n",
       "      <td>0.300304</td>\n",
       "      <td>0.183816</td>\n",
       "      <td>0.096076</td>\n",
       "      <td>0.10040</td>\n",
       "      <td>0.07460</td>\n",
       "      <td>0.049440</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.06615</td>\n",
       "      <td>...</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.070586</td>\n",
       "      <td>0.13630</td>\n",
       "      <td>0.16440</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.078870</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>0.07732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.344976</td>\n",
       "      <td>0.434224</td>\n",
       "      <td>0.345380</td>\n",
       "      <td>0.206278</td>\n",
       "      <td>0.10380</td>\n",
       "      <td>0.11540</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.061390</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.05982</td>\n",
       "      <td>...</td>\n",
       "      <td>15.29</td>\n",
       "      <td>0.593017</td>\n",
       "      <td>0.268390</td>\n",
       "      <td>0.133479</td>\n",
       "      <td>0.13800</td>\n",
       "      <td>0.27330</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>0.08351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.331251</td>\n",
       "      <td>0.335137</td>\n",
       "      <td>0.327068</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06544</td>\n",
       "      <td>...</td>\n",
       "      <td>17.04</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.168133</td>\n",
       "      <td>0.16130</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.127550</td>\n",
       "      <td>0.115996</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.22040</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.070380</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>0.09575</td>\n",
       "      <td>...</td>\n",
       "      <td>10.60</td>\n",
       "      <td>0.160448</td>\n",
       "      <td>0.094925</td>\n",
       "      <td>0.035121</td>\n",
       "      <td>0.20060</td>\n",
       "      <td>0.36630</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.2848</td>\n",
       "      <td>0.13640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.285342</td>\n",
       "      <td>0.423064</td>\n",
       "      <td>0.264114</td>\n",
       "      <td>0.162418</td>\n",
       "      <td>0.06251</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.05234</td>\n",
       "      <td>...</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.188107</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.03432</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.05843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>0.268261</td>\n",
       "      <td>0.159788</td>\n",
       "      <td>0.08369</td>\n",
       "      <td>0.05073</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.05449</td>\n",
       "      <td>...</td>\n",
       "      <td>14.34</td>\n",
       "      <td>0.529318</td>\n",
       "      <td>0.202450</td>\n",
       "      <td>0.108951</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.10930</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.059210</td>\n",
       "      <td>0.2306</td>\n",
       "      <td>0.06291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.228075</td>\n",
       "      <td>0.232330</td>\n",
       "      <td>0.243245</td>\n",
       "      <td>0.122375</td>\n",
       "      <td>0.10910</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.074150</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.07371</td>\n",
       "      <td>...</td>\n",
       "      <td>13.74</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.206783</td>\n",
       "      <td>0.099907</td>\n",
       "      <td>0.13850</td>\n",
       "      <td>0.40920</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.5774</td>\n",
       "      <td>0.10300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0       0.334564      0.589787        0.328865   0.193807          0.09929   \n",
       "1       0.412656      0.358471        0.396724   0.264305          0.09597   \n",
       "2       0.268304      0.286101        0.268813   0.145111          0.10760   \n",
       "3       0.187373      0.300304        0.183816   0.096076          0.10040   \n",
       "4       0.344976      0.434224        0.345380   0.206278          0.10380   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "210     0.331251      0.335137        0.327068   0.193425          0.10600   \n",
       "211     0.127550      0.115996        0.140488   0.054719          0.12550   \n",
       "212     0.285342      0.423064        0.264114   0.162418          0.06251   \n",
       "213     0.284869      0.521474        0.268261   0.159788          0.08369   \n",
       "214     0.228075      0.232330        0.243245   0.122375          0.10910   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.11260        0.044620             0.043040         0.1537   \n",
       "1             0.08799        0.065930             0.051890         0.1618   \n",
       "2             0.13340        0.080170             0.050740         0.1641   \n",
       "3             0.07460        0.049440             0.029320         0.1486   \n",
       "4             0.11540        0.146300             0.061390         0.1926   \n",
       "..                ...             ...                  ...            ...   \n",
       "210           0.11330        0.112600             0.064630         0.1669   \n",
       "211           0.22040        0.118800             0.070380         0.2057   \n",
       "212           0.01938        0.001595             0.001852         0.1395   \n",
       "213           0.05073        0.012060             0.017620         0.1667   \n",
       "214           0.17000        0.165900             0.074150         0.2678   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.06171  ...         15.30       0.563699   \n",
       "1                   0.05549  ...         20.11       0.554371   \n",
       "2                   0.06854  ...         14.38       0.269989   \n",
       "3                   0.06615  ...         12.40       0.361407   \n",
       "4                   0.05982  ...         15.29       0.593017   \n",
       "..                      ...  ...           ...            ...   \n",
       "210                 0.06544  ...         17.04       0.500533   \n",
       "211                 0.09575  ...         10.60       0.160448   \n",
       "212                 0.05234  ...         14.00       0.453092   \n",
       "213                 0.05449  ...         14.34       0.529318   \n",
       "214                 0.07371  ...         13.74       0.382729   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0           0.247971    0.128170           0.12410            0.22640   \n",
       "1           0.392898    0.266368           0.14140            0.35470   \n",
       "2           0.223517    0.110229           0.15330            0.38420   \n",
       "3           0.161114    0.070586           0.13630            0.16440   \n",
       "4           0.268390    0.133479           0.13800            0.27330   \n",
       "..               ...         ...               ...                ...   \n",
       "210         0.316201    0.168133           0.16130            0.35680   \n",
       "211         0.094925    0.035121           0.20060            0.36630   \n",
       "212         0.188107    0.104109           0.08125            0.03432   \n",
       "213         0.202450    0.108951           0.12180            0.10930   \n",
       "214         0.206783    0.099907           0.13850            0.40920   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0           0.132600              0.104800          0.2250   \n",
       "1           0.290200              0.154100          0.3437   \n",
       "2           0.358200              0.140700          0.3230   \n",
       "3           0.141200              0.078870          0.2251   \n",
       "4           0.423400              0.136200          0.2698   \n",
       "..               ...                   ...             ...   \n",
       "210         0.406900              0.182700          0.3179   \n",
       "211         0.291300              0.107500          0.2848   \n",
       "212         0.007977              0.009259          0.2295   \n",
       "213         0.044620              0.059210          0.2306   \n",
       "214         0.450400              0.186500          0.5774   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.08321  \n",
       "1                    0.08631  \n",
       "2                    0.10330  \n",
       "3                    0.07732  \n",
       "4                    0.08351  \n",
       "..                       ...  \n",
       "210                  0.10550  \n",
       "211                  0.13640  \n",
       "212                  0.05843  \n",
       "213                  0.06291  \n",
       "214                  0.10300  \n",
       "\n",
       "[215 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unmportant features for both X Train and X Test.\n",
    "X_train_df = X_train_df[top_twenty_features]\n",
    "X_test_df = X_test_df[top_twenty_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>radius error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.402717</td>\n",
       "      <td>0.405017</td>\n",
       "      <td>0.255016</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.15620</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.09113</td>\n",
       "      <td>0.6470</td>\n",
       "      <td>4.675</td>\n",
       "      <td>0.112268</td>\n",
       "      <td>0.04972</td>\n",
       "      <td>0.01639</td>\n",
       "      <td>21.20</td>\n",
       "      <td>0.463486</td>\n",
       "      <td>0.456646</td>\n",
       "      <td>0.288488</td>\n",
       "      <td>0.1681</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.5553</td>\n",
       "      <td>0.21210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.607175</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.473595</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.14790</td>\n",
       "      <td>0.09498</td>\n",
       "      <td>0.7582</td>\n",
       "      <td>5.865</td>\n",
       "      <td>0.197233</td>\n",
       "      <td>0.03391</td>\n",
       "      <td>0.01521</td>\n",
       "      <td>27.32</td>\n",
       "      <td>0.502665</td>\n",
       "      <td>0.679267</td>\n",
       "      <td>0.543846</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>0.23880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.448625</td>\n",
       "      <td>0.452699</td>\n",
       "      <td>0.292428</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.17930</td>\n",
       "      <td>0.08866</td>\n",
       "      <td>0.3037</td>\n",
       "      <td>2.482</td>\n",
       "      <td>0.046298</td>\n",
       "      <td>0.05371</td>\n",
       "      <td>0.01813</td>\n",
       "      <td>17.79</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>0.364012</td>\n",
       "      <td>0.195635</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.20350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.296228</td>\n",
       "      <td>0.297699</td>\n",
       "      <td>0.169417</td>\n",
       "      <td>0.08284</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.02833</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>3.369</td>\n",
       "      <td>0.031767</td>\n",
       "      <td>0.07683</td>\n",
       "      <td>0.01368</td>\n",
       "      <td>15.44</td>\n",
       "      <td>0.359275</td>\n",
       "      <td>0.321679</td>\n",
       "      <td>0.134757</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.5646</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>0.13570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247480</td>\n",
       "      <td>0.241794</td>\n",
       "      <td>0.135101</td>\n",
       "      <td>0.08108</td>\n",
       "      <td>0.07823</td>\n",
       "      <td>0.06839</td>\n",
       "      <td>0.02534</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>2.097</td>\n",
       "      <td>0.024576</td>\n",
       "      <td>0.04344</td>\n",
       "      <td>0.01087</td>\n",
       "      <td>13.13</td>\n",
       "      <td>0.193763</td>\n",
       "      <td>0.185467</td>\n",
       "      <td>0.084718</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3076</td>\n",
       "      <td>0.09140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.267358</td>\n",
       "      <td>0.265082</td>\n",
       "      <td>0.142906</td>\n",
       "      <td>0.09933</td>\n",
       "      <td>0.12090</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>0.06021</td>\n",
       "      <td>0.3424</td>\n",
       "      <td>2.711</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>0.05101</td>\n",
       "      <td>0.02295</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.358475</td>\n",
       "      <td>0.192191</td>\n",
       "      <td>0.084103</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.11050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.323678</td>\n",
       "      <td>0.335429</td>\n",
       "      <td>0.191898</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.16810</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.06759</td>\n",
       "      <td>0.4751</td>\n",
       "      <td>2.974</td>\n",
       "      <td>0.060232</td>\n",
       "      <td>0.03476</td>\n",
       "      <td>0.01616</td>\n",
       "      <td>16.01</td>\n",
       "      <td>0.557569</td>\n",
       "      <td>0.276856</td>\n",
       "      <td>0.148152</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.3966</td>\n",
       "      <td>0.3381</td>\n",
       "      <td>0.15210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.352075</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.211665</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.09388</td>\n",
       "      <td>0.05839</td>\n",
       "      <td>0.2895</td>\n",
       "      <td>2.376</td>\n",
       "      <td>0.037445</td>\n",
       "      <td>0.03321</td>\n",
       "      <td>0.01424</td>\n",
       "      <td>16.33</td>\n",
       "      <td>0.502132</td>\n",
       "      <td>0.294288</td>\n",
       "      <td>0.157589</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.3026</td>\n",
       "      <td>0.3194</td>\n",
       "      <td>0.15650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.252686</td>\n",
       "      <td>0.242278</td>\n",
       "      <td>0.135992</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.2360</td>\n",
       "      <td>1.670</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.01683</td>\n",
       "      <td>0.01241</td>\n",
       "      <td>13.50</td>\n",
       "      <td>0.096482</td>\n",
       "      <td>0.182081</td>\n",
       "      <td>0.089437</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>0.09391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.385205</td>\n",
       "      <td>0.380001</td>\n",
       "      <td>0.243097</td>\n",
       "      <td>0.08876</td>\n",
       "      <td>0.09588</td>\n",
       "      <td>0.07550</td>\n",
       "      <td>0.04079</td>\n",
       "      <td>0.2711</td>\n",
       "      <td>1.974</td>\n",
       "      <td>0.036679</td>\n",
       "      <td>0.02039</td>\n",
       "      <td>0.00826</td>\n",
       "      <td>17.77</td>\n",
       "      <td>0.219083</td>\n",
       "      <td>0.335126</td>\n",
       "      <td>0.197675</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.3327</td>\n",
       "      <td>0.12520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean perimeter  mean area  mean smoothness  \\\n",
       "0       0.402717        0.405017   0.255016          0.11600   \n",
       "1       0.607175        0.595743   0.473595          0.09831   \n",
       "2       0.448625        0.452699   0.292428          0.09831   \n",
       "3       0.296228        0.297699   0.169417          0.08284   \n",
       "4       0.247480        0.241794   0.135101          0.08108   \n",
       "..           ...             ...        ...              ...   \n",
       "486     0.267358        0.265082   0.142906          0.09933   \n",
       "487     0.323678        0.335429   0.191898          0.11620   \n",
       "488     0.352075        0.350287   0.211665          0.09752   \n",
       "489     0.252686        0.242278   0.135992          0.10280   \n",
       "490     0.385205        0.380001   0.243097          0.08876   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  radius error  \\\n",
       "0             0.15620         0.18910              0.09113        0.6470   \n",
       "1             0.10270         0.14790              0.09498        0.7582   \n",
       "2             0.15560         0.17930              0.08866        0.3037   \n",
       "3             0.12230         0.10100              0.02833        0.2810   \n",
       "4             0.07823         0.06839              0.02534        0.2666   \n",
       "..                ...             ...                  ...           ...   \n",
       "486           0.12090         0.10650              0.06021        0.3424   \n",
       "487           0.16810         0.13570              0.06759        0.4751   \n",
       "488           0.11410         0.09388              0.05839        0.2895   \n",
       "489           0.06981         0.03987              0.03700        0.2360   \n",
       "490           0.09588         0.07550              0.04079        0.2711   \n",
       "\n",
       "     perimeter error  area error  concavity error  concave points error  \\\n",
       "0              4.675    0.112268          0.04972               0.01639   \n",
       "1              5.865    0.197233          0.03391               0.01521   \n",
       "2              2.482    0.046298          0.05371               0.01813   \n",
       "3              3.369    0.031767          0.07683               0.01368   \n",
       "4              2.097    0.024576          0.04344               0.01087   \n",
       "..               ...         ...              ...                   ...   \n",
       "486            2.711    0.025547          0.05101               0.02295   \n",
       "487            2.974    0.060232          0.03476               0.01616   \n",
       "488            2.376    0.037445          0.03321               0.01424   \n",
       "489            1.670    0.019851          0.01683               0.01241   \n",
       "490            1.974    0.036679          0.02039               0.00826   \n",
       "\n",
       "     worst radius  worst texture  worst perimeter  worst area  \\\n",
       "0           21.20       0.463486         0.456646    0.288488   \n",
       "1           27.32       0.502665         0.679267    0.543846   \n",
       "2           17.79       0.437900         0.364012    0.195635   \n",
       "3           15.44       0.359275         0.321679    0.134757   \n",
       "4           13.13       0.193763         0.185467    0.084718   \n",
       "..            ...            ...              ...         ...   \n",
       "486         13.33       0.358475         0.192191    0.084103   \n",
       "487         16.01       0.557569         0.276856    0.148152   \n",
       "488         16.33       0.502132         0.294288    0.157589   \n",
       "489         13.50       0.096482         0.182081    0.089437   \n",
       "490         17.77       0.219083         0.335126    0.197675   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0              0.1681             0.3913           0.5553   \n",
       "1              0.1512             0.3150           0.5372   \n",
       "2              0.1415             0.4667           0.5862   \n",
       "3              0.1201             0.5646           0.6556   \n",
       "4              0.1026             0.2431           0.3076   \n",
       "..                ...                ...              ...   \n",
       "486            0.1287             0.2250           0.2216   \n",
       "487            0.1794             0.3966           0.3381   \n",
       "488            0.1431             0.3026           0.3194   \n",
       "489            0.1385             0.1266           0.1242   \n",
       "490            0.1491             0.3331           0.3327   \n",
       "\n",
       "     worst concave points  \n",
       "0                 0.21210  \n",
       "1                 0.23880  \n",
       "2                 0.20350  \n",
       "3                 0.13570  \n",
       "4                 0.09140  \n",
       "..                    ...  \n",
       "486               0.11050  \n",
       "487               0.15210  \n",
       "488               0.15650  \n",
       "489               0.09391  \n",
       "490               0.12520  \n",
       "\n",
       "[491 rows x 20 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>radius error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.334564</td>\n",
       "      <td>0.328865</td>\n",
       "      <td>0.193807</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.3645</td>\n",
       "      <td>2.888</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>0.020710</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>15.30</td>\n",
       "      <td>0.563699</td>\n",
       "      <td>0.247971</td>\n",
       "      <td>0.128170</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.412656</td>\n",
       "      <td>0.396724</td>\n",
       "      <td>0.264305</td>\n",
       "      <td>0.09597</td>\n",
       "      <td>0.08799</td>\n",
       "      <td>0.065930</td>\n",
       "      <td>0.051890</td>\n",
       "      <td>0.3699</td>\n",
       "      <td>2.406</td>\n",
       "      <td>0.063837</td>\n",
       "      <td>0.019540</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>20.11</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>0.392898</td>\n",
       "      <td>0.266368</td>\n",
       "      <td>0.14140</td>\n",
       "      <td>0.35470</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268304</td>\n",
       "      <td>0.268813</td>\n",
       "      <td>0.145111</td>\n",
       "      <td>0.10760</td>\n",
       "      <td>0.13340</td>\n",
       "      <td>0.080170</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>1.696</td>\n",
       "      <td>0.021662</td>\n",
       "      <td>0.026360</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>14.38</td>\n",
       "      <td>0.269989</td>\n",
       "      <td>0.223517</td>\n",
       "      <td>0.110229</td>\n",
       "      <td>0.15330</td>\n",
       "      <td>0.38420</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.187373</td>\n",
       "      <td>0.183816</td>\n",
       "      <td>0.096076</td>\n",
       "      <td>0.10040</td>\n",
       "      <td>0.07460</td>\n",
       "      <td>0.049440</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>3.018</td>\n",
       "      <td>0.035447</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.070586</td>\n",
       "      <td>0.13630</td>\n",
       "      <td>0.16440</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.078870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.344976</td>\n",
       "      <td>0.345380</td>\n",
       "      <td>0.206278</td>\n",
       "      <td>0.10380</td>\n",
       "      <td>0.11540</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.061390</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>1.895</td>\n",
       "      <td>0.021924</td>\n",
       "      <td>0.046450</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>15.29</td>\n",
       "      <td>0.593017</td>\n",
       "      <td>0.268390</td>\n",
       "      <td>0.133479</td>\n",
       "      <td>0.13800</td>\n",
       "      <td>0.27330</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.331251</td>\n",
       "      <td>0.327068</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>1.602</td>\n",
       "      <td>0.022503</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>17.04</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.168133</td>\n",
       "      <td>0.16130</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.127550</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.22040</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.070380</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>1.787</td>\n",
       "      <td>0.020299</td>\n",
       "      <td>0.051890</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>10.60</td>\n",
       "      <td>0.160448</td>\n",
       "      <td>0.094925</td>\n",
       "      <td>0.035121</td>\n",
       "      <td>0.20060</td>\n",
       "      <td>0.36630</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.285342</td>\n",
       "      <td>0.264114</td>\n",
       "      <td>0.162418</td>\n",
       "      <td>0.06251</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.1731</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.188107</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.03432</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.009259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.268261</td>\n",
       "      <td>0.159788</td>\n",
       "      <td>0.08369</td>\n",
       "      <td>0.05073</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>1.657</td>\n",
       "      <td>0.026873</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>14.34</td>\n",
       "      <td>0.529318</td>\n",
       "      <td>0.202450</td>\n",
       "      <td>0.108951</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.10930</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.059210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.228075</td>\n",
       "      <td>0.243245</td>\n",
       "      <td>0.122375</td>\n",
       "      <td>0.10910</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.074150</td>\n",
       "      <td>0.3197</td>\n",
       "      <td>2.281</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>0.046490</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>13.74</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.206783</td>\n",
       "      <td>0.099907</td>\n",
       "      <td>0.13850</td>\n",
       "      <td>0.40920</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.186500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean perimeter  mean area  mean smoothness  \\\n",
       "0       0.334564        0.328865   0.193807          0.09929   \n",
       "1       0.412656        0.396724   0.264305          0.09597   \n",
       "2       0.268304        0.268813   0.145111          0.10760   \n",
       "3       0.187373        0.183816   0.096076          0.10040   \n",
       "4       0.344976        0.345380   0.206278          0.10380   \n",
       "..           ...             ...        ...              ...   \n",
       "210     0.331251        0.327068   0.193425          0.10600   \n",
       "211     0.127550        0.140488   0.054719          0.12550   \n",
       "212     0.285342        0.264114   0.162418          0.06251   \n",
       "213     0.284869        0.268261   0.159788          0.08369   \n",
       "214     0.228075        0.243245   0.122375          0.10910   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  radius error  \\\n",
       "0             0.11260        0.044620             0.043040        0.3645   \n",
       "1             0.08799        0.065930             0.051890        0.3699   \n",
       "2             0.13340        0.080170             0.050740        0.2324   \n",
       "3             0.07460        0.049440             0.029320        0.3796   \n",
       "4             0.11540        0.146300             0.061390        0.2027   \n",
       "..                ...             ...                  ...           ...   \n",
       "210           0.11330        0.112600             0.064630        0.2208   \n",
       "211           0.22040        0.118800             0.070380        0.2744   \n",
       "212           0.01938        0.001595             0.001852        0.1731   \n",
       "213           0.05073        0.012060             0.017620        0.2621   \n",
       "214           0.17000        0.165900             0.074150        0.3197   \n",
       "\n",
       "     perimeter error  area error  concavity error  concave points error  \\\n",
       "0              2.888    0.043030         0.020710              0.016260   \n",
       "1              2.406    0.063837         0.019540              0.009767   \n",
       "2              1.696    0.021662         0.026360              0.010320   \n",
       "3              3.018    0.035447         0.019900              0.011550   \n",
       "4              1.895    0.021924         0.046450              0.012760   \n",
       "..               ...         ...              ...                   ...   \n",
       "210            1.602    0.022503         0.021850              0.009567   \n",
       "211            1.787    0.020299         0.051890              0.014500   \n",
       "212            1.101    0.014079         0.001595              0.001852   \n",
       "213            1.657    0.026873         0.005681              0.006336   \n",
       "214            2.281    0.033467         0.046490              0.018430   \n",
       "\n",
       "     worst radius  worst texture  worst perimeter  worst area  \\\n",
       "0           15.30       0.563699         0.247971    0.128170   \n",
       "1           20.11       0.554371         0.392898    0.266368   \n",
       "2           14.38       0.269989         0.223517    0.110229   \n",
       "3           12.40       0.361407         0.161114    0.070586   \n",
       "4           15.29       0.593017         0.268390    0.133479   \n",
       "..            ...            ...              ...         ...   \n",
       "210         17.04       0.500533         0.316201    0.168133   \n",
       "211         10.60       0.160448         0.094925    0.035121   \n",
       "212         14.00       0.453092         0.188107    0.104109   \n",
       "213         14.34       0.529318         0.202450    0.108951   \n",
       "214         13.74       0.382729         0.206783    0.099907   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.12410            0.22640         0.132600   \n",
       "1             0.14140            0.35470         0.290200   \n",
       "2             0.15330            0.38420         0.358200   \n",
       "3             0.13630            0.16440         0.141200   \n",
       "4             0.13800            0.27330         0.423400   \n",
       "..                ...                ...              ...   \n",
       "210           0.16130            0.35680         0.406900   \n",
       "211           0.20060            0.36630         0.291300   \n",
       "212           0.08125            0.03432         0.007977   \n",
       "213           0.12180            0.10930         0.044620   \n",
       "214           0.13850            0.40920         0.450400   \n",
       "\n",
       "     worst concave points  \n",
       "0                0.104800  \n",
       "1                0.154100  \n",
       "2                0.140700  \n",
       "3                0.078870  \n",
       "4                0.136200  \n",
       "..                    ...  \n",
       "210              0.182700  \n",
       "211              0.107500  \n",
       "212              0.009259  \n",
       "213              0.059210  \n",
       "214              0.186500  \n",
       "\n",
       "[215 rows x 20 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the final x train and x test values\n",
    "X_train_final = X_train_df.values\n",
    "X_test_final = X_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. The numbers add up just fine ðŸ‘Œ......Next up is model building ðŸ˜Ž\n",
    "\n",
    "\n",
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=400)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=400)\n",
    "\n",
    "#training the Logistic Regression model using Training data\n",
    "\n",
    "model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data =  0.9572301425661914\n"
     ]
    }
   ],
   "source": [
    "# accuracy on training data\n",
    "X_train_prediction = model.predict(X_train_final)\n",
    "training_data_accuracy = accuracy_score(y_train, X_train_prediction)\n",
    "print('Accuracy on training data = ', training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data =  0.9302325581395349\n"
     ]
    }
   ],
   "source": [
    "# accuracy on test data\n",
    "X_test_prediction = model.predict(X_test_final)\n",
    "test_data_accuracy = accuracy_score(y_test, X_test_prediction)\n",
    "print('Accuracy on test data = ', test_data_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use K Fold cross validation to measure accuracy of our Logistic Regression model\n",
    "\n",
    "This is just for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97902098, 0.98601399, 0.97902098, 0.95804196, 0.95104895])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit #it randomizes  our samples to ensure each fold has equal distribution\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "cross_val_score(LogisticRegression(max_iter=400), X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706293706293707"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the mean score for all the predictions the model made\n",
    "cross_val_score(LogisticRegression(max_iter=400), X, y, cv=cv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, even cross validation yields simillar accuracy scores for our model. Note that for cross validation we did not use X_train and X_test which had our top 20 features rather we used all the features the dataset has. This is solely for comparison purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization & Best Model Selection\n",
    "\n",
    "\n",
    "#### RandomizedSearchCV\n",
    "We typically use RandomizedSearchCV to reduce number of iterations and with random combination of parameters. This is useful when you have too many parameters to try and your training time is longer. It helps reduce the cost of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.955166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.966384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.973397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_C  mean_test_score\n",
       "0       1         0.955166\n",
       "1       5         0.966384\n",
       "2      10         0.973397"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rs = RandomizedSearchCV(LogisticRegression(solver='liblinear',multi_class='auto'), {\n",
    "        'C': [1,5,10]\n",
    "    }, \n",
    "    cv=5, \n",
    "    return_train_score=False, \n",
    "    n_iter=3\n",
    ")\n",
    "rs.fit(X, y)\n",
    "pd.DataFrame(rs.cv_results_)[['param_C','mean_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our model can perform slightly better when we set the C parameter to 5 or 10 but how do about different models with different hyperparameters perform? Lets find out below.\n",
    "\n",
    "\n",
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto',random_state=42),\n",
    "        'params' : {\n",
    "            'C': [1,10,20],\n",
    "            'kernel': ['rbf','linear']\n",
    "        }  \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params' : {\n",
    "            'n_estimators': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "    'decision_tree' : {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'criterion': ['entropy','gini'],\n",
    "            'splitter' : ['best','random']\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "          'model': LogisticRegression(solver='liblinear',multi_class='auto',random_state=42),\n",
    "          'params': {\n",
    "              'C': [1,5,10]\n",
    "          }\n",
    "      }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.980380</td>\n",
       "      <td>{'C': 20, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.979001</td>\n",
       "      <td>{'n_estimators': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.974766</td>\n",
       "      <td>{'criterion': 'entropy', 'splitter': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>0.973397</td>\n",
       "      <td>{'C': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  best_score  \\\n",
       "0                  svm    0.980380   \n",
       "1        random_forest    0.979001   \n",
       "2        decision_tree    0.974766   \n",
       "3  logistic_regression    0.973397   \n",
       "\n",
       "                                      best_params  \n",
       "0                   {'C': 20, 'kernel': 'linear'}  \n",
       "1                            {'n_estimators': 10}  \n",
       "2  {'criterion': 'entropy', 'splitter': 'random'}  \n",
       "3                                       {'C': 10}  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
    "    clf.fit(X, y)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all these models have a lot of hyperparameters we can use to train them. GridSearchCV utilizes combinations to decide on the best set of hyperparameters under the hood hence experimenting with all of them will be time consuming. Given that our model performance is not that bad, it is best we leave the optimazation here and go with random forest as our classifier. ðŸ––ðŸ‘Š\n",
    "\n",
    "\n",
    "### Best Optimized Model & Indepth Model Performance Evaluation\n",
    "\n",
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5, random_state=15)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model = RandomForestClassifier(n_estimators=5,random_state=15)\n",
    "\n",
    "# training the model using Training data\n",
    "optimal_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data =  0.9898167006109979\n"
     ]
    }
   ],
   "source": [
    "# accuracy on training data\n",
    "optimal_X_train_prediction = optimal_model.predict(X_train_final)\n",
    "optimal_training_data_accuracy = accuracy_score(y_train, optimal_X_train_prediction)\n",
    "print('Accuracy on training data = ', optimal_training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data =  0.9720930232558139\n"
     ]
    }
   ],
   "source": [
    "# accuracy on test data\n",
    "optimal_X_test_prediction = optimal_model.predict(X_test_final)\n",
    "optimal_test_data_accuracy = accuracy_score(y_test, optimal_X_test_prediction)\n",
    "print('Accuracy on test data = ', optimal_test_data_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears our model has a balanced fit. This notebook was executed several times and each time it was executed both the training accuracy and testing accuracy were not so further off. It is also worth noting that since random forest is a stochastic algorithm this behavior of it having different accuracy scores on different datasets was expected and for that reason we are going to keep this iteration as the last one and evaluate our model with it. Hence, in the future, do not be alarmed when this pipeline is reexecuted and we get different results on accuracy. It is a feature not a bug. ðŸ˜ðŸ˜ƒðŸ˜†ðŸƒâ€â™‚ï¸ðŸƒâ€â™€ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Let us plot a confusion matrix for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2006f093bc8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(optimal_model, \n",
    "                      X_test_final, \n",
    "                      y_test, \n",
    "                      display_labels=[\"Benign\",\"Malignant\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the model classified 107 (105+2) observations as benign. This classification equates to 98.1% accuracy. 105/107 of those which were picked as benign were correctly classified. For the sensitivity, 108(4+104) daignoses where classified as malignant. From that, 108, 104 of them were correctly classified, yielding a 96.3% classification accuracy i.e. (104/108)*100 = 96.3%\n",
    "\n",
    "\n",
    "\n",
    "To Break this confusion matrix further we have:\n",
    "\n",
    "##### Specificity aka True Negative Rate\n",
    "105 True Negatives (TN): Diagnoses which are benign and correctly classifed as benign\n",
    "\n",
    "2 False Positives (FP): Diagnoses which are benign but were wrongly classfied as malignant\n",
    "\n",
    "Specifity = TN/TN+FP\n",
    "\n",
    "Specificity = 105/105+2\n",
    "\n",
    "Specifity = 0.981 or 98.1%\n",
    "\n",
    "\n",
    "##### Sensitivity\n",
    "104 True Positives (TP): Diagnoses which are malignant and correctly classifed as malignant\n",
    "\n",
    "4 False Negatives (FN): Diagnoses which are malignant but were wrongly classfied as benign\n",
    "\n",
    "Sensitivity = TP/TP+FN\n",
    "\n",
    "Sensitivity = 104/104+4\n",
    "\n",
    "Sensitivity = 0.963 or 96.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic(ROC) & Area Under The Curve (AUC)\n",
    "\n",
    "The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the â€˜signalâ€™ from the â€˜noiseâ€™. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the probabilities of our classes instead of predicting the target labels/classes\n",
    "y_train_score = optimal_model.predict_proba(X_train_final)[:, 1]\n",
    "y_test_score = optimal_model.predict_proba(X_test_final)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of Predictions\n",
    "A machine learning classification model can be used to predict the actual class of the data point directly or predict its probability of belonging to different classes. The latter gives us more control over the result. We can determine our own threshold to interpret the result of the classifier. This is sometimes more prudent than just building a completely new model!\n",
    "\n",
    "Setting different thresholds for classifying positive class for data points will inadvertently change the Sensitivity and Specificity of the model. And one of these thresholds will probably give a better result than the others, depending on whether we are aiming to lower the number of False Negatives or False Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (roc_curve, roc_auc_score, auc,classification_report)\n",
    "auc_train = roc_auc_score(y_train, y_train_score)\n",
    "auc_test = roc_auc_score(y_test, y_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Training AUC: 0.9995934824450787\n",
      "      Testing AUC: 0.9907407407407407\n"
     ]
    }
   ],
   "source": [
    "#display AUC score for training and testing\n",
    "print(f\"\"\"\n",
    "      Training AUC: {auc_train}\n",
    "      Testing AUC: {auc_test}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the false positive rate and true positive rate\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_score)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPR tells us what proportion of the negative class got incorrectly classified.\n",
    "\n",
    "TPR tells us what proportion of the positive class got correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When AUC = 1, then the classifier is able to perfectly distinguish between all the benign and the malignant class points correctly. If, however, the AUC had been 0, then the classifier would be predicting all benign as malignant, and all malignant as benign.\n",
    "\n",
    "\n",
    "When 0.5<AUC<1, there is a high chance that the classifier will be able to distinguish the malignant class values from the benign class values. This is so because the classifier is able to detect more numbers of True positives and True negatives than False negatives and False positives.\n",
    "\n",
    "\n",
    "When AUC=0.5, then the classifier is not able to distinguish between malignant and benign class points. Meaning either the classifier is predicting random class or constant class for all the data points.\n",
    "\n",
    "Our AUC score is 0.99 which means our model is a doing a stellar job in differentiating the two classes.The higher the AUC, the better the performance of the model at distinguishing between benign and and malignant classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = optimal_model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for element in yp:\n",
    "    if element > 0.5: #this threshold can be different\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97       107\n",
      "         1.0       0.98      0.96      0.97       108\n",
      "\n",
      "    accuracy                           0.97       215\n",
      "   macro avg       0.97      0.97      0.97       215\n",
      "weighted avg       0.97      0.97      0.97       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision\n",
    "Precision talks about how precise/accurate our model is out of those predicted positive,how many of them are actual positive\n",
    "\n",
    "Precision = TP/TP+FP\n",
    "\n",
    "\n",
    "##### Recall  aka Sensitivity aka True Positive Rate\n",
    "Recall calculates how many of the actual positives our model capture through labeling it as Positive (True Positive). Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative. Like in this context.Therefore\n",
    "\n",
    "Recall = TP/TP+FN\n",
    "\n",
    "\n",
    "##### F1 Score\n",
    "F1 is a function of Precision and Recall. We need it to seek a balance between Precision and Recall. \n",
    "\n",
    "All in all, this will be a good model if we were interested in idenfying benign diagnoses. It would do a stellar job. However the 3% (based on the f1-score and accuracy) and 2% (recall score for the malignant class) misclassifications could be problamatic. Especially if we do not admistered timely treatements to patients who do have cancer but our model says they are just faking it ðŸ™„. Cancer can be life threatening if not treated early and maliganant tumors can also metastasize to other parts of the body. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Predictive System\n",
    "\n",
    "Lets export our dataset to a text file to use for our model testing. Remember that we dropped some features when doing feature selection? Great, then we have to export that last modified data frame. This is the same dataset we used for testing our model. Feel free to use your own for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_test_df['label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>radius error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>...</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.334564</td>\n",
       "      <td>0.328865</td>\n",
       "      <td>0.193807</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.3645</td>\n",
       "      <td>2.888</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>15.30</td>\n",
       "      <td>0.563699</td>\n",
       "      <td>0.247971</td>\n",
       "      <td>0.128170</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.412656</td>\n",
       "      <td>0.396724</td>\n",
       "      <td>0.264305</td>\n",
       "      <td>0.09597</td>\n",
       "      <td>0.08799</td>\n",
       "      <td>0.065930</td>\n",
       "      <td>0.051890</td>\n",
       "      <td>0.3699</td>\n",
       "      <td>2.406</td>\n",
       "      <td>0.063837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>20.11</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>0.392898</td>\n",
       "      <td>0.266368</td>\n",
       "      <td>0.14140</td>\n",
       "      <td>0.35470</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268304</td>\n",
       "      <td>0.268813</td>\n",
       "      <td>0.145111</td>\n",
       "      <td>0.10760</td>\n",
       "      <td>0.13340</td>\n",
       "      <td>0.080170</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>1.696</td>\n",
       "      <td>0.021662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>14.38</td>\n",
       "      <td>0.269989</td>\n",
       "      <td>0.223517</td>\n",
       "      <td>0.110229</td>\n",
       "      <td>0.15330</td>\n",
       "      <td>0.38420</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.187373</td>\n",
       "      <td>0.183816</td>\n",
       "      <td>0.096076</td>\n",
       "      <td>0.10040</td>\n",
       "      <td>0.07460</td>\n",
       "      <td>0.049440</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>3.018</td>\n",
       "      <td>0.035447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.070586</td>\n",
       "      <td>0.13630</td>\n",
       "      <td>0.16440</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.078870</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.344976</td>\n",
       "      <td>0.345380</td>\n",
       "      <td>0.206278</td>\n",
       "      <td>0.10380</td>\n",
       "      <td>0.11540</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.061390</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>1.895</td>\n",
       "      <td>0.021924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>15.29</td>\n",
       "      <td>0.593017</td>\n",
       "      <td>0.268390</td>\n",
       "      <td>0.133479</td>\n",
       "      <td>0.13800</td>\n",
       "      <td>0.27330</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.331251</td>\n",
       "      <td>0.327068</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>1.602</td>\n",
       "      <td>0.022503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>17.04</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.168133</td>\n",
       "      <td>0.16130</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.127550</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.22040</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.070380</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>1.787</td>\n",
       "      <td>0.020299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>10.60</td>\n",
       "      <td>0.160448</td>\n",
       "      <td>0.094925</td>\n",
       "      <td>0.035121</td>\n",
       "      <td>0.20060</td>\n",
       "      <td>0.36630</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.285342</td>\n",
       "      <td>0.264114</td>\n",
       "      <td>0.162418</td>\n",
       "      <td>0.06251</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.1731</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.188107</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.03432</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.268261</td>\n",
       "      <td>0.159788</td>\n",
       "      <td>0.08369</td>\n",
       "      <td>0.05073</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>1.657</td>\n",
       "      <td>0.026873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>14.34</td>\n",
       "      <td>0.529318</td>\n",
       "      <td>0.202450</td>\n",
       "      <td>0.108951</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.10930</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.059210</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.228075</td>\n",
       "      <td>0.243245</td>\n",
       "      <td>0.122375</td>\n",
       "      <td>0.10910</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.074150</td>\n",
       "      <td>0.3197</td>\n",
       "      <td>2.281</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>13.74</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.206783</td>\n",
       "      <td>0.099907</td>\n",
       "      <td>0.13850</td>\n",
       "      <td>0.40920</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean perimeter  mean area  mean smoothness  \\\n",
       "0       0.334564        0.328865   0.193807          0.09929   \n",
       "1       0.412656        0.396724   0.264305          0.09597   \n",
       "2       0.268304        0.268813   0.145111          0.10760   \n",
       "3       0.187373        0.183816   0.096076          0.10040   \n",
       "4       0.344976        0.345380   0.206278          0.10380   \n",
       "..           ...             ...        ...              ...   \n",
       "210     0.331251        0.327068   0.193425          0.10600   \n",
       "211     0.127550        0.140488   0.054719          0.12550   \n",
       "212     0.285342        0.264114   0.162418          0.06251   \n",
       "213     0.284869        0.268261   0.159788          0.08369   \n",
       "214     0.228075        0.243245   0.122375          0.10910   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  radius error  \\\n",
       "0             0.11260        0.044620             0.043040        0.3645   \n",
       "1             0.08799        0.065930             0.051890        0.3699   \n",
       "2             0.13340        0.080170             0.050740        0.2324   \n",
       "3             0.07460        0.049440             0.029320        0.3796   \n",
       "4             0.11540        0.146300             0.061390        0.2027   \n",
       "..                ...             ...                  ...           ...   \n",
       "210           0.11330        0.112600             0.064630        0.2208   \n",
       "211           0.22040        0.118800             0.070380        0.2744   \n",
       "212           0.01938        0.001595             0.001852        0.1731   \n",
       "213           0.05073        0.012060             0.017620        0.2621   \n",
       "214           0.17000        0.165900             0.074150        0.3197   \n",
       "\n",
       "     perimeter error  area error  ...  concave points error  worst radius  \\\n",
       "0              2.888    0.043030  ...              0.016260         15.30   \n",
       "1              2.406    0.063837  ...              0.009767         20.11   \n",
       "2              1.696    0.021662  ...              0.010320         14.38   \n",
       "3              3.018    0.035447  ...              0.011550         12.40   \n",
       "4              1.895    0.021924  ...              0.012760         15.29   \n",
       "..               ...         ...  ...                   ...           ...   \n",
       "210            1.602    0.022503  ...              0.009567         17.04   \n",
       "211            1.787    0.020299  ...              0.014500         10.60   \n",
       "212            1.101    0.014079  ...              0.001852         14.00   \n",
       "213            1.657    0.026873  ...              0.006336         14.34   \n",
       "214            2.281    0.033467  ...              0.018430         13.74   \n",
       "\n",
       "     worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0         0.563699         0.247971    0.128170           0.12410   \n",
       "1         0.554371         0.392898    0.266368           0.14140   \n",
       "2         0.269989         0.223517    0.110229           0.15330   \n",
       "3         0.361407         0.161114    0.070586           0.13630   \n",
       "4         0.593017         0.268390    0.133479           0.13800   \n",
       "..             ...              ...         ...               ...   \n",
       "210       0.500533         0.316201    0.168133           0.16130   \n",
       "211       0.160448         0.094925    0.035121           0.20060   \n",
       "212       0.453092         0.188107    0.104109           0.08125   \n",
       "213       0.529318         0.202450    0.108951           0.12180   \n",
       "214       0.382729         0.206783    0.099907           0.13850   \n",
       "\n",
       "     worst compactness  worst concavity  worst concave points  label  \n",
       "0              0.22640         0.132600              0.104800    1.0  \n",
       "1              0.35470         0.290200              0.154100    0.0  \n",
       "2              0.38420         0.358200              0.140700    1.0  \n",
       "3              0.16440         0.141200              0.078870    1.0  \n",
       "4              0.27330         0.423400              0.136200    0.0  \n",
       "..                 ...              ...                   ...    ...  \n",
       "210            0.35680         0.406900              0.182700    0.0  \n",
       "211            0.36630         0.291300              0.107500    1.0  \n",
       "212            0.03432         0.007977              0.009259    1.0  \n",
       "213            0.10930         0.044620              0.059210    1.0  \n",
       "214            0.40920         0.450400              0.186500    0.0  \n",
       "\n",
       "[215 rows x 21 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df.to_csv('Breast Cancer Classfication Testing Dataset.txt', header=None, index=None, sep=',', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "The Breast cancer is Malignant\n"
     ]
    }
   ],
   "source": [
    "input_data = (0.5659993374035686,0.5515168267569622,0.41845174973488863,0.09009,0.1029,0.108,0.07951,0.7888,5.486,96.05,0.02269,0.0137,24.86,0.3880597014925373,0.5751780467154739,0.413094769956744,0.1193,0.2336,0.2687,0.1789)\n",
    "\n",
    "# change the input data to a numpy array\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "\n",
    "# reshape the numpy array as we are predicting for one datapoint\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "prediction = model.predict(input_data_reshaped)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0] == 0):\n",
    "  print('The Breast cancer is Malignant')\n",
    "\n",
    "else:\n",
    "  print('The Breast Cancer is Benign')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the tested model to a pickle file\n",
    "\n",
    "Model building is complete, lets export it to a pickle file to used by our python FastAPI sever, React Js and React Native Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('breast_cancer_classification_model.pickle','wb') as f:\n",
    "    pickle.dump(optimal_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
